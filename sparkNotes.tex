\documentclass{article}
\usepackage{graphicx}
\usepackage{dot2texi}
\makeatletter
\@ifundefined{verbatim@out}{\newwrite\verbatim@out}{}
\makeatother
\usepackage{tikz}
\usepackage{hyperref}
\usetikzlibrary{shapes,arrows}
% \usepackage[pdf]{graphviz}
%\usepackage{feynmp}
\usepackage{subfigure}
\usepackage{dsfont}
\graphicspath{{figs/}}

\title{Deep Learning Notes}

\author{Peter Thompson}

\begin{document}

\section{Questions}
\begin{itemize}
    \item {\bf Tasks/cores/executors}
    \begin{itemize}
        \item in databricks, can we specify only 2 (for instance) executors for a node with 8 cpus?
        \item if so, are these executors/jvms aware that the node has more cpus available? Are they accessible
        Use case - want to run some non-spark multithreaded code, e.g. distribute 1 xgboost job to each node, with each job using c cores
        \item Ali said that he turned off autoscaling and had one executor per node. (8 cores per node)
    \end{itemize}
    \item {\bf joining} 
        We have (say) a small list of collector ids, and want to left join with 5-6 large tables (millions of collector ids, thousands of columns)
        What is the best (optimal) way to do this? More shuffle partitions are better? more data partitions?  
    \item {\bf joining also} - Build/stream? what are these? is this for hashing?

    \end{itemize}

\section{Joining}
    \subsection{links}
    \begin{itemize}
        \item \url{https://databricks.com/session/optimizing-apache-spark-sql-joins} 30 minute video, good overview
        \item \url{https://sujithjay.com/spark-sql/2018/02/17/Broadcast-Hash-Joins-in-Apache-Spark/} part one of a series
    \end{itemize}

    \subsection{Shuffle Hash join}
    Pretty (most?) common type of join. Data from table 1 and table 2 are partitioned by key. Matching partitions are sent to the same executor, then the output is merged.
    This works best when
    \begin{itemize}
        \item data is evenly distributed by key
        \item good number of keys (unique values - monthid would be a bad key (12 unique values))
    \end{itemize}
    When these conditions can't be satisfied, then broadcast hash join might be suitable
    NOTE spark config option,\verb|spark.sql.join.preferSortMergeJoin|, (set true by default), essentially means that Sort Merge join is always chosen over shuffle hash join


    Diagnosing problems
    \begin{itemize}
        \item Tasks that take a long time - could be uneven sharding (keys not distributed evenly).
        \item Speculative tasks - if something takes a long time on one exdecutor, spark might send think theres a problem there and send the task to another to see if ti works there. If one particular task spawns speculative tasks, this can also be a sign that there is a sharding problem.
        \item Shuffle read/write memory - take a look to see if there is one task sending/receiving a lot of data (more than average)
        \item ls across the output files - see if one data partition is much larger than others
    \end{itemize}

    \subsection{broadcast hash join}
    Small table sent to every executor, each partition is joined with the entire table, then output is returned. Bestest performance. Spark will automatically broadcast tables that are smaller than \verb|spark.sql.autoBroadcastJoinThreshold|, which is 10 MB by default. Giving hints can override this behaviour, but if the broadcasted table is too big you may OOM your cluster.
    For right outer join, Spark can only broadcast the left side. For left outer, left semi, left anti and the internal join type ExistenceJoin, Spark can only broadcast the right side. I have had cases where I want to join a small list of collectors, tableA, (one column of collector keys) to a bigger table (millions of rows, thousands of columns). Normally I would do \verb|c = tableA.join(tableB,['COLLECTOR_KEY'],how='left')|. To use broadcasting, the join has to be righted: \verb|c = tableB.join(broadcast(tableA),['COLLECTOR_KEY'],how='right')|. 

    \verb|df.explain()| on the output will tell whether a shuffle hash or broadcast hash join is being made
    (spark can sometimes have a hard time figuring out when to use a broadcast join with stuff in hive. giving a hint might be handy).

\subsection{Cartesian join (cross join)}
\begin{itemize}
\item create an rdd of uid by uid pairs
\item broadcast that
\item call a udf that retrieves data from the big tables given the uid,uid pairs
\end{itemize}

\subsection{one to many join (one sided cartesian)}
- number of rows can explode
not a big del with parquet - duplicate data encodes well (for output files)

\subsection{theta join}
join tableA with TableB on (keyA < keyB +10)
  - Spark will treat this as a cartesian join
  - Much better to bucket A and B, and create an intial paritioning based on bucket equality

\section{rough}

parallelism 

Transformations
narrow transformations are where each input partition contributes to one output partition.
Spark pipelines stuff - in narrow transformations, things like filters are all performed in memory.
Wide transformations are those where a SHUFFLE is required, an output partition requires data from multiple input partitions.
Shuffles transfer data across the network, and results are written to disk.

lazy evaluation
spark doesn't do anything untill you need a result. Predicate pushdown - if you filter at different stages, these get pushed down as far as possible, so spark only deals with the minimum amount of data required. This allows for lots of optimisation

Actions
There are three types of actions
- actions to view datsa in the console (.show(), display in databricks)
- actions to collect results at driver (collect())
- actions to write data (write)

Explain can be called on any dataframe. It will show the physical plan for processing the data to carry out all the transformations. The top of the plan is the end result, the bottom are the inputs. The logical plan is the set of transformations specified, the physical plan is what spark will do to get the result, 


###################################
berkely amp lab

started by Matei Zaharia. Was working on netflix prize using mapreduce, tried to make it better and invented spark, tied for netflix prize, but ended up second.
certification

catalyst optimiser - spark dataframes can be 5-20X faster than regular RDDs
yet another resource negotiater

Performance - dataframe performance is really good, and there is no difference between python, scala, R or SQL. When working with RDDs, Scala performs better than Python (about 2x)
Catalyst - binds python (or R or SQL) dataframe functionality to scala based operations. Fast

architecture - driver -> executor -> slot. Typically worker nodes have one excutor, and that has one slot (process thread) per cpu available


Jobs are made of stages
each stage is the set of operations that can be run in parallel without dependencies (shuffling)

databricks - REPL shell
displayHTML - can pretty display things


anytime you create a spark session (dataframe interface stuff), a spark context named sc is created automagically

documentation for spark - python and scala are maintained by different people, check scala docs if python ones seem to be lacking

jobs created when spark needs to touch data
When you spark.read, this is not an action
a job is created, and run eagerly, on spark.read. This is to get an idea of the schema

nullable things in schema aren't used by spark, but are to maintain compatibility with other databases (parquet?)

parquet
google dremel project - released white paper, desxcribes file format
parquet derived from this
preserves dataframe info - both metadata and row/column data

read.parquet executes (eagerly) one very small spark job to read metadata. read.csv reads entire file (all data)
can specify the schema even for parquet files, this avoids the (small) job to read metadata

# Dataframes
anything that returns not a dataframe is an action. triggers evaluation

dataset - rdd of type specific stuff. Stringly typed. Collection of classes.
Not available in python, only have dataframes. Dataframes are datasets of row objects. One row may have different columns to another. 

Caching - when an action is called, spark figures out the lineage (physical plan? logical plan?) and builds the dag. When it hits some data that's been cached then it uses that and does not go back further.


#Transformations and actions
Narrow transformatoins are those that do not require a shuffle. The transformation can happen within each partition
Wide transformations require data from other partitions. These require a shuffle
A stage is a set of jobs that happen within partitions. A set of narrow transformations. 
green dots in the dag are cache points

Catalyst Optimiser
sequence of api calls and action -> unresolved logical plan -> check catalog (from hive, make sure all columns exist) -> resolved logical plan -> optimised logical plan (first pass, move filters down) -> series of physical plans (lots of paths to get the same result) -> check cost of each plan -> choose best -> do stuff, result (runs on rdds)
udfs can screw up optimisation.
catalyst assumes udfs run in jvm (judf is in java or scala )
python udfs are particulary difficult. Data may need to be taken out of the jvm, run through a python interpreter, put back into jvm and back into dag.
python udfs are bad.

https://spark-summit.org/2016/events/deep-dive-into-catalyst-apache-spark-20s-optimizer
https://www.youtube.com/watch?v=6bCpISym_0w
https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html
http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf



Wide and Narrow
Wide transformations shuffle data. If we are using 4 shuffle partitions, then for each datapartition we hash data into 4 partitions, and write out a file for each partition. Then we send each file to the corresponding executor.  GetPartition is the function that figures out which shuffle partition a record goes into


Cache can screw things up. If data is cached in the middle of a pipeline, then spark can't push predicates(filters) down past there. In some cases, removing the cache means the filter can move right to the beginning, which speeds up a bunch of stuff

spark will combine as many narrow operations as it can into a single task

Caching before a shuffle is pointless. Spark will track back from the action to the shuffle, and get to the files on disk, those sent to the node on the second part of the shuffle. It will never go back through the shuffle to reach the cache. The transformations and actions notebook has some of this

*** Jobs/Actions - one job per RDD ACTION. Catalyst will take the dataframe code (API), run it throguh catalyst and get an optimised logical plan (which is formed in terms of transformations and actions on the underlying RDD). There is one spark job per RDD action in the optimised plan. So a sequence of dataframe transformations may result in a single (dataframe) action, which is optimised to multiple RDD actions, resulting in multiple jobs





\end{document}


