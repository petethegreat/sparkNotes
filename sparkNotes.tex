\documentclass{article}
\usepackage{graphicx}
\usepackage{dot2texi}
\makeatletter
\@ifundefined{verbatim@out}{\newwrite\verbatim@out}{}
\makeatother
\usepackage{tikz}
\usepackage{hyperref}
\usetikzlibrary{shapes,arrows}
% \usepackage[pdf]{graphviz}
%\usepackage{feynmp}
\usepackage{subfigure}
\usepackage{dsfont}
\graphicspath{{figs/}}

\title{Deep Learning Notes}

\author{Peter Thompson}

\begin{document}

\section{Questions}
\begin{itemize}
    \item {\bf Tasks/cores/executors}
    \begin{itemize}
        \item in databricks, can we specify only 2 (for instance) executors for a node with 8 cpus?
        \item if so, are these executors/jvms aware that the node has more cpus available? Are they accessible
        Use case - want to run some non-spark multithreaded code, e.g. distribute 1 xgboost job to each node, with each job using c cores
        \item Ali said that he turned off autoscaling and had one executor per node. (8 cores per node)
    \end{itemize}
    \item {\bf joining} 
        We have (say) a small list of collector ids, and want to left join with 5-6 large tables (millions of collector ids, thousands of columns)
        What is the best (optimal) way to do this? More shuffle partitions are better? more data partitions?  
    \item {\bf joining also} - Build/stream? what are these? is this for hashing?

    \end{itemize}

\section{Joining}
    \subsection{links}
    \begin{itemize}
        \item \url{https://databricks.com/session/optimizing-apache-spark-sql-joins} 30 minute video, good overview
        \item \url{https://sujithjay.com/spark-sql/2018/02/17/Broadcast-Hash-Joins-in-Apache-Spark/} part one of a series
    \end{itemize}

    \subsection{Shuffle Hash join}
    Pretty (most?) common type of join. Data from table 1 and table 2 are partitioned by key. Matching partitions are sent to the same executor, then the output is merged.
    This works best when
    \begin{itemize}
        \item data is evenly distributed by key
        \item good number of keys (unique values - monthid would be a bad key (12 unique values))
    \end{itemize}
    When these conditions can't be satisfied, then broadcast hash join might be suitable
    NOTE spark config option,\verb|spark.sql.join.preferSortMergeJoin|, (set true by default), essentially means that Sort Merge join is always chosen over shuffle hash join


    Diagnosing problems
    \begin{itemize}
        \item Tasks that take a long time - could be uneven sharding (keys not distributed evenly).
        \item Speculative tasks - if something takes a long time on one exdecutor, spark might send think theres a problem there and send the task to another to see if ti works there. If one particular task spawns speculative tasks, this can also be a sign that there is a sharding problem.
        \item Shuffle read/write memory - take a look to see if there is one task sending/receiving a lot of data (more than average)
        \item ls across the output files - see if one data partition is much larger than others
    \end{itemize}

    \subsection{broadcast hash join}
    Small table sent to every executor, each partition is joined with the entire table, then output is returned. Bestest performance. Spark will automatically broadcast tables that are smaller than \verb|spark.sql.autoBroadcastJoinThreshold|, which is 10 MB by default. Giving hints can override this behaviour, but if the broadcasted table is too big you may OOM your cluster.
    For right outer join, Spark can only broadcast the left side. For left outer, left semi, left anti and the internal join type ExistenceJoin, Spark can only broadcast the right side. I have had cases where I want to join a small list of collectors, tableA, (one column of collector keys) to a bigger table (millions of rows, thousands of columns). Normally I would do \verb|c = tableA.join(tableB,['COLLECTOR_KEY'],how='left')|. To use broadcasting, the join has to be righted: \verb|c = tableB.join(broadcast(tableA),['COLLECTOR_KEY'],how='right')|. 

    \verb|df.explain()| on the output will tell whether a shuffle hash or broadcast hash join is being made
    (spark can sometimes have a hard time figuring out when to use a broadcast join with stuff in hive. giving a hint might be handy).

\subsection{Cartesian join (cross join)}
\begin{itemize}
\item create an rdd of uid by uid pairs
\item broadcast that
\item call a udf that retrieves data from the big tables given the uid,uid pairs
\end{itemize}

\subsection{one to many join (one sided cartesian)}
- number of rows can explode
not a big del with parquet - duplicate data encodes well (for output files)

\subsection{theta join}
join tableA with TableB on (keyA < keyB +10)
  - Spark will treat this as a cartesian join
  - Much better to bucket A and B, and create an intial paritioning based on bucket equality

\section{rough}

parallelism 

Transformations
narrow transformations are where each input partition contributes to one output partition.
Spark pipelines stuff - in narrow transformations, things like filters are all performed in memory.
Wide transformations are those where a SHUFFLE is required, an output partition requires data from multiple input partitions.
Shuffles transfer data across the network, and results are written to disk.

lazy evaluation
spark doesn't do anything untill you need a result. Predicate pushdown - if you filter at different stages, these get pushed down as far as possible, so spark only deals with the minimum amount of data required. This allows for lots of optimisation

Actions
There are three types of actions
- actions to view datsa in the console (.show(), display in databricks)
- actions to collect results at driver (collect())
- actions to write data (write)

Explain can be called on any dataframe. It will show the physical plan for processing the data to carry out all the transformations. The top of the plan is the end result, the bottom are the inputs. The logical plan is the set of transformations specified, the physical plan is what spark will do to get the result, 


###################################
berkely amp lab

started by Matei Zaharia. Was working on netflix prize using mapreduce, tried to make it better and invented spark, tied for netflix prize, but ended up second.
certification

catalyst optimiser - spark dataframes can be 5-20X faster than regular RDDs
yet another resource negotiater

Performance - dataframe performance is really good, and there is no difference between python, scala, R or SQL. When working with RDDs, Scala performs better than Python (about 2x)
Catalyst - binds python (or R or SQL) dataframe functionality to scala based operations. Fast

architecture - driver -> executor -> slot. Typically worker nodes have one excutor, and that has one slot (process thread) per cpu available


Jobs are made of stages
each stage is the set of operations that can be run in parallel without dependencies (shuffling)

databricks - ripple shell?
displayHTML - can pretty display things


anytime you create a spark session (dataframe interface stuff), a spark context named sc is created automagically

documentation for spark - python and scala are maintained by different people, check scala docs if python ones seem to be lacking

jobs created when spark needs to touch data
When you spark.read, this is not an action
a job is created, and run eagerly, on spark.read. This is to get an idea of the schema

nullable things in schema aren't used by spark, but are to maintain compatibility with other databases (parquet?)

parquet
google dremel project - released white paper, desxcribes file format
parquet derived from this
preserves dataframe info - both metadata and row/column data

read.parquet executes (eagerly) one very small spark job to read metadata. read.csv reads entire file (all data)
can specify the schema even for parquet files, this avoids the (small) job to read metadata


\end{document}


